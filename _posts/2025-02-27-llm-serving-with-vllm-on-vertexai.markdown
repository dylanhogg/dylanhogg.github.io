---
layout: post
title: "Practical Large Language Model Serving with vLLM on Google VertexAI"
subtitle: "Easy, Fast, and Cheap LLM Serving for Everyone"
date: 2025-02-27 09:00:00
tags: [vLLM, VertexAI, LLM, Serving, Deployment, Machine Learning, NLP]
author: "Dylan Hogg"
header-img: "img/post-bg-04.jpg"
published: true
comments: true
---

<style>
    .center {
      width: 60%;
      margin: 0 auto;
    }

    /* substack copy pasta css hacks: */
    /*  also, replace class="image-caption caption" with class="image-caption caption" */
    .captioned-image-container {
      width: 60%;
      margin: 0 auto;
    }
    .img-responsive {
      width: 100%;
      margin: 0 auto;
    }
    .image-caption {
      width: 80%;
      margin: 0 auto;
      font-size: smaller;
      color: #777;
    }
</style>

<p>If you're looking to deploy high-performance LLMs on Google Vertex AI, this post will show you how to leverage vLLM's speed and scalability with a few simple deployment steps using Google’s custom vLLM docker images.</p>

<h2 class="header-anchor-post">What is vLLM?</h2>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png" width="1456" height="417" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:417,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;vllm-logo-text-light.png (3000×860)&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="vllm-logo-text-light.png (3000×860)" title="vllm-logo-text-light.png (3000×860)" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f7f07-9be4-4c6c-9afd-0af129acc1d7_3000x860.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div>
<p>vLLM is an efficient and high-performance inference and serving framework for many types of large language models (LLMs). It is designed to maximise throughput and minimise memory usage, making it particularly useful for deploying LLM’s in production.</p>
<p>It can be used both as:</p>
<ul><li><p>a Python library (e.g. <samp>import vllm</samp>)</p></li><li><p><span>or as a CLI server (e.g. <samp>vllm serve Qwen/Qwen2.5-7B-Instruct --args</samp>), with an option to adhere to the common </span><a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html" rel="nofollow ugc noopener">OpenAI API standard</a><span>.</span></p></li></ul>
<p><span>vLLM seamlessly supports many models on </span><a href="https://huggingface.co/models" rel="nofollow ugc noopener">Hugging Face</a><span>, including:</span></p>
<ul><li><p>Transformer-like LLMs (e.g., Llama)</p></li><li><p>Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)</p></li><li><p>Embedding Models (e.g. E5-Mistral)</p></li><li><p>Multi-modal LLMs (e.g., LLaVA)</p></li></ul>
<p><span>Originally developed in the </span><a href="https://sky.cs.berkeley.edu/" rel="nofollow ugc noopener">Sky Computing Lab</a><span> at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.</span></p>
<p><span>The team behind vLLM started with a new KV cache algorithm called </span><a href="https://arxiv.org/abs/2309.06180" rel="nofollow ugc noopener">PagedAttention</a><span>, and also built the open-source server: </span><a href="https://github.com/vllm-project/vllm" rel="nofollow ugc noopener">https://github.com/vllm-project/vllm</a></p>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png" width="1456" height="634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:258808,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26fa4b1f-7562-40e4-b32a-6fbd7f30dad3_1548x674.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"></div></div></a><figcaption class="image-caption caption"><a href="https://arxiv.org/pdf/2309.06180" rel="nofollow ugc noopener">https://arxiv.org/pdf/2309.06180</a></figcaption></figure></div>
<p><span>In Dec 2024 vLLM became a PyTorch ecosystem project, and </span><a href="https://pytorch.org/blog/vllm-joins-pytorch/" rel="nofollow ugc noopener">joined the PyTorch ecosystem family</a><span>!</span></p>

<h2 class="header-anchor-post">Why is vLLM fast? 🚀</h2>
<p>vLLM has state-of-the-art serving throughput, with configurable options for:</p>
<ul><li><p>Efficient management of attention key-value (KV) cache memory using PagedAttention</p></li><li><p>PagedAttention is a memory-efficient mechanism used to optimise KV cache (and is orthogonal to other caching mechanisms like FlashAttention)</p></li><li><p>Continuous batching of incoming requests</p></li><li><p>Speculative decoding, a technique designed to speed up text generation by predicting multiple tokens in advance and verifying them with a draft model and reducing latency</p></li><li><p>Chunked prefill - an inference optimisation technique that improves GPU utilisation during prefill (processing input tokens before generation begins)</p></li><li><p>Model quantization support: GPTQ, AWQ, INT4, INT8, and FP8</p></li><li><p>Optimised CUDA kernels, including integration with FlashAttention</p></li><li><p>Tensor parallelism and distributed serving on multiple GPUs</p></li></ul>
<p>vLLM isn’t the only kid in town, here are some alternative high performance inference engines:</p>
<ul><li><p><a href="https://github.com/NVIDIA/TensorRT-LLM" rel="nofollow ugc noopener">TensorRT-LLM by NVIDIA</a></p></li><li><p><a href="https://github.com/huggingface/text-generation-inference" rel="nofollow ugc noopener">Text Generation Inference by HuggingFace</a></p></li><li><p><a href="https://github.com/deepspeedai/DeepSpeed" rel="nofollow ugc noopener">DeepSpeed by Microsoft</a></p></li><li><p><a href="https://github.com/ggml-org/llama.cpp" rel="nofollow ugc noopener">llama.cpp</a></p></li></ul>

<h2 class="header-anchor-post">Google’s VertexAI API I/O interface</h2>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png" width="1456" height="560" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:560,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:273923,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2cd090-bc1b-491d-bdd4-eaaea4de64ad_1462x562.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div>
<p><span>vLLM can be deployed on a variety of GPU hardware and cloud options including </span><a href="https://medium.com/towards-data-science/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124" rel="nofollow ugc noopener">AWS SageMaker</a><span>, </span><a href="https://techcommunity.microsoft.com/blog/machinelearningblog/unlocking-function-calling-with-vllm-and-azure-machine-learning/4362457" rel="nofollow ugc noopener">Microsoft Azure</a><span>, on-prem, and many other providers.</span></p>
<p><span>In this post we’ll look specifically at deploying a Hugging Face or custom trained model with vLLM to Google’s </span><a href="https://cloud.google.com/vertex-ai?hl=en#deploy-a-model-for-production-use" rel="nofollow ugc noopener">Vertex AI platform</a><span>, and how to work around the fixed Vertex AI API interface.</span></p>
<p><span>As you may have seen in the VertexAI endpoint test UI, the API specifies a very </span><a href="https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict#request-body" rel="nofollow ugc noopener">specific schema</a><span> for API requests and responses when making predictions:</span></p>
<p>VertexAI prediction request schema:</p>
<pre><code>{
    "instances": [ "&lt;instances here&gt;" ]
}</code></pre>
<p>VertexAI prediction response schema:</p>
<pre><code>{
    "deployedModelId": "123412341234",
    "model": "projects/456745674567/locations/us-central1/models/678967896789",
    "modelDisplayName": "my-custom-model-vllm-qwen25-1pt5b-inst",
    "modelVersionId": "1",
    "predictions": [ "&lt;predictions here&gt;" ]
}</code></pre>
<p><span>vLLM however doesn't natively support this request/response schema with “instances” and “predictions” keys, instead it's commonly deployed with the </span><a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html" rel="nofollow ugc noopener">OpenAI API interface</a><span>. So what should we do?</span></p>

<h2 class="header-anchor-post">GCP vLLM Docker image</h2>
<p><span>Luckily Google publish </span><a href="https://console.cloud.google.com/artifacts/docker/vertex-ai/us/vertex-vision-model-garden-dockers/pytorch-vllm-serve" rel="nofollow ugc noopener">custom vLLM server docker images</a><span> for us to use! Thanks google.</span></p>
<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png" width="300" height="168" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:168,&quot;width&quot;:300,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a6ef5e-445d-4cac-a310-ceeecd9b34da_300x168.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div>
<p>At the time of writing (early Feb 2025) the latest image tag is <samp>us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250202_0916_RC00</samp> (at 12.8 GB) which if you bash into it, you will find the vLLM version <samp>0.1.dev1+g4f51006.d20250202</samp> (via <samp>cat /workspace/vllm/vllm/_version.py</samp>)</p>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png" width="833" height="327" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a70c391e-d07b-4687-934b-54bffd20aff9_833x327.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:327,&quot;width&quot;:833,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:54567,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c391e-d07b-4687-934b-54bffd20aff9_833x327.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption class="image-caption caption"><a href="https://console.cloud.google.com/artifacts/docker/vertex-ai/us/vertex-vision-model-garden-dockers/pytorch-vllm-serve" rel="nofollow ugc noopener">us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve</a></figcaption></figure></div>
<p><span>This doesn't match any vLLM </span><a href="https://github.com/vllm-project/vllm/releases" rel="nofollow ugc noopener">release versions</a><span>, and a possible reason is that Google patch vLLM (using something similar to </span><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/8c527a89edb5b7a283391cd9f5e52480bc662577/community-content/vertex_model_garden/model_oss/vllm/vllm.patch" rel="nofollow ugc noopener">this patch file</a><span>). The closest I have found is <samp>cat /workspace/vllm/README.md</samp> and looking at the date of the most recent "Latest News", which for tag <samp>20250202_0916_RC00</samp> is 2024/11 which would match </span><a href="https://github.com/vllm-project/vllm/blob/v0.6.4/README.md" rel="nofollow ugc noopener">https://github.com/vllm-project/vllm/blob/v0.6.4/README.md</a><span>)</span></p>
<p>These customisations add several VertexAI compatibility features:</p>
<ul><li><p><span>Support for downloading a custom model from </span><a href="https://cloud.google.com/storage" rel="nofollow ugc noopener">Google Cloud Storage</a><span> (GCS) via a <samp>gs://&lt;bucket&gt;/&lt;path&gt;/&lt;model&gt;</samp> specifier</span></p></li><li><p><span>Support for downloading a custom model from </span><a href="https://aws.amazon.com/s3/" rel="nofollow ugc noopener">AWS S3</a><span> via <samp>s3://&lt;bucket&gt;/&lt;path&gt;/&lt;model&gt;</samp></span></p></li><li><p>A way to call the chat/completions endpoint or the generate endpoint (determined by adding "@requestFormat": "chatCompletions" to the instance)</p></li><li><p><samp>/ping</samp> endpoint required for Vertex deployment</p></li></ul>
<p>Given this image, we can easily deploy it to VertexAI as a model, and specify any vLLM server args to control its operation.</p>

<h3 class="header-anchor-post">Deploy a Hugging Face model to a VertexAI endpoint</h3>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png" width="1456" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:531383,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3adcff61-05a9-4302-b64d-cba523201a3b_2160x594.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div>
<p></p>
<p><span>For example, here we deploy vLLM with the Hugging Face </span><a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" rel="nofollow ugc noopener">Qwen/Qwen2.5-1.5B-Instruct</a><span> model and server args <samp>--max-model-len=4156,--gpu-memory-utilization=0.95,--disable-log-stats</samp>:</span></p>
<pre><code>MODEL_NAME="Qwen/Qwen2.5-1.5B-Instruct"
IMAGE_NAME=my-custom-model-vllm-qwen25-1pt5b-inst

REPOSITORY="us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve"
REPOSITORY_BUILD="20250202_0916_RC00"

# Upload VertexAI model

gcloud ai models upload \
 --project="$PROJECT_NAME" \
  --region="$REGION" \
 --display-name="$IMAGE_NAME" \
  --container-image-uri="$REPOSITORY:$REPOSITORY_BUILD" \
  --container-command="python,-m,vllm.entrypoints.api_server" \
  --container-args="--host=0.0.0.0,--port=7080,--model=$MODEL_NAME,--max-model-len=4156,--gpu-memory-utilization=0.95,--disable-log-stats" \
 --container-ports=7080 \
 --container-health-route="/ping" \
 --container-predict-route="/generate"

# Deploy to a VertexAI endpoint, specifying machine and GPU type:

gcloud ai endpoints deploy-model "$ENDPOINT_ID" \
    --project="$PROJECT_NAME" \
 --model="$MODEL_ID" \
    --display-name="$IMAGE_NAME" \
 --region="$REGION" \
 --min-replica-count=1 \
 --max-replica-count=1 \
 --traffic-split=0=100 \
 --machine-type=g2-standard-4 \
 --accelerator=type=nvidia-l4,count=1 \
 --enable-access-logging</code></pre>

<h3 class="header-anchor-post">Deploy a custom model from Cloud Storage</h3>
<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png" width="729" height="212" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:212,&quot;width&quot;:729,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37740,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf75c6e5-419e-43ef-b0e1-159e7860911c_729x212.png 1456w" sizes="100vw" loading="lazy"></picture><div></div></div></a></figure></div>
<p><span>If you want to deploy a custom model (e.g. a merged base-model with finetuned </span><a href="https://huggingface.co/docs/peft/main/en/conceptual_guides/lora" rel="nofollow ugc noopener">LoRA</a><span> weights) from a GCS bucket, you only need to specify the <samp>--artifact-uri</samp> gcloud argument pointing to your bucket, and omit the <samp>--model=$MODEL_NAME</samp> argument to vLLM. </span></p>
<p><span>Specifying <samp>--artifact-uri</samp> will set the AIP_STORAGE_URI environment variable to be an internal Google managed GCS bucket name (e.g. <samp>gs://caip-tenant-/2051094961151016960/artifacts</samp>) which is a copy of the user specified bucket, and a model value of None will ensure the patched google Docker image attempts to load from <samp>AIP_STORAGE_URI</samp>. </span><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/8c527a89edb5b7a283391cd9f5e52480bc662577/community-content/vertex_model_garden/model_oss/vllm/vllm.patch#L168" rel="nofollow ugc noopener">Reference source code</a><span> as I couldn't find actual docs on this feature.</span></p>
<p>Deploy a custom model, as described in the previous paragraph:</p>
<pre><code>MODEL_ARTIFACT_URI="gs://&lt;bucket&gt;/&lt;path&gt;/Qwen-Qwen2.5-1.5B-Instruct/model-merged"
IMAGE_NAME=my-custom-model-vllm-gcs

gcloud ai models upload \
 --project="$PROJECT_NAME" \
  --region="$REGION" \
 --display-name="$IMAGE_NAME" \
  --container-image-uri="$REPOSITORY:$REPOSITORY_BUILD" \
  --container-command="python,-m,vllm.entrypoints.api_server" \
  --container-args="--host=0.0.0.0,--port=7080,--disable-log-stats" \
  --container-ports=7080 \
  --container-health-route="/ping" \
  --container-predict-route="/generate" \
  --artifact-uri="$MODEL_ARTIFACT_URI"</code></pre>

<p>It's worth noting that we're not locked into VertexAI with vLLM, here are some Kubernetes resources for GKE or platform-agnostic k8s cluster deployment:</p>
<ul><li><p><a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-multiple-gpu" rel="nofollow ugc noopener">https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-multiple-gpu</a></p></li><li><p><a href="https://github.com/vllm-project/production-stack?tab=readme-ov-file#deployment" rel="nofollow ugc noopener">https://github.com/vllm-project/production-stack?tab=readme-ov-file#deployment</a></p></li></ul>

<h2 class="header-anchor-post">How do I call the model?</h2>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c33f6db-0b36-44a0-bd66-d3998478e923_1920x1080.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div>
<p>Although vLLM has the OpenAI API, since the server is wrapped, we need to call it like like any VertexAI endpoint with the “instances” key:</p>
<p>VertexAI endpoint request example:</p>
<pre><code>request_body = {
"instances": [
    {
      "@requestFormat": "chatCompletions",
      "messages": [
        {
          "role": "system",
          "content": "&lt;system_prompt&gt;"
        },
        {
          "role": "user",
          "content": "&lt;user_content&gt;"
        }
      ],
      "max_tokens": 4096,
      "temperature": 0.1
    },
    { "&lt;more instances here" }
  ]
}

headers = {
"Authorization": f"Bearer {get_access_token()}",
"Content-Type": "application/json",
}

endpoint_url = f"https://{region}-aiplatform.googleapis.com/v1/projects/{project_name}/locations/{region}/endpoints/{endpoint_id}:predict"
endpoint_timeout = 30
response = requests.post(endpoint_url, headers=headers, json=request_body, timeout=endpoint_timeout)</code></pre>

<p>The prediction response matches the order of the input instances - there is no "id" field to join back to get input/output results, just use the list index.</p>
<p><span>Also note that if you exclude <samp>"@requestFormat": "chatCompletions"</samp> then you will call the generate completion and not the chat completion. (</span><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/e02adc516cbdff199f70da753d2d4b84c6c17b0f/community-content/vertex_model_garden/model_oss/vllm/vllm.patch#L286" rel="nofollow ugc noopener">Example patch code showing this branch</a><span>). For a refresher on the difference between Chat vs. Completion, see </span><a href="https://generallyintelligent.substack.com/p/chat-vs-completion-endpoints" rel="nofollow ugc noopener">this blogpost</a><span>.</span></p>

<h2 class="header-anchor-post">What performance metrics matter?</h2>
<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png" width="652" height="219" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:219,&quot;width&quot;:652,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30936,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0ee2950-b8a4-4dbc-8f8e-e962d4467fc1_652x219.png 1456w" sizes="100vw" loading="lazy"></picture><div></div></div></a></figure></div>
<p><strong>Latency</strong><span>: Elapsed time of an event </span><strong>Throughput</strong><span>: The number of events that can be executed per unit of time</span></p>
<p>Throughput vs. Latency Restaurant Analogy</p>
<p><strong>Latency</strong><span>: how long it takes to prepare and serve a single meal after a customer places an order. If the chef focuses on one order at a time, it gets served quickly (low latency), but the number of customers served per hour is low.</span></p>
<p><strong>Throughput</strong><span>: total number of meals the kitchen can prepare and serve per hour. If the chef prepares multiple orders together in batches, the kitchen can serve more people in total (high throughput), but each individual order may take longer to be completed.</span></p>
<p><span>The specifics of which Latency vs Throughput metrics are "good" depends on the project requirements and how the user will interact with the LLM. vLLM is engineered to efficiently balance throughput and latency in production settings by leveraging </span><a href="https://www.hyperstack.cloud/blog/case-study/what-is-vllm-a-guide-to-quick-inference#toc-continuous-batching-to-maximise-throughput" rel="nofollow ugc noopener">continuous batching</a><span> and an </span><a href="https://docs.vllm.ai/en/latest/design/arch_overview.html#asyncllmengine" rel="nofollow ugc noopener">asynchronous engine</a></p>
<p><span>According to an </span><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference#benchmarking-results:-throughput" rel="nofollow ugc noopener">AnyScale benchmark</a><span>, on an NVIDIA A100 GPU with 40GB of GPU RAM, continuous batching with vLLM achieved 6,121 tokens/sec when max-tokens was set to 32, and 1,898 tokens/sec when max-tokens was set to 1,536. The same benchmark also </span><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference#benchmarking-results:-latency" rel="nofollow ugc noopener">reports</a><span> vLLM to have the lowest latency compared to other frameworks and claim that continuous batching also improves median latency.</span></p>
<p><span>Additional performance notes can be found in this blog post: </span><a href="https://blog.vllm.ai/2024/09/05/perf-update.html" rel="nofollow ugc noopener">https://blog.vllm.ai/2024/09/05/perf-update.html</a></p>

<h2 class="header-anchor-post"><strong>Which models can we serve on an L4 GPU with 32GB RAM?</strong></h2>
<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png" width="1456" height="708" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:708,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:969207,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" class="sizing-normal img-responsive" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a21a66-d8df-40dc-9cf6-fee64d73e645_1876x912.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div>
<p><span>Here is a brief set of results regarding the Qwen2.5 series of models on a single machine configuration of: a </span><a href="https://gcloud-compute.com/g2-standard-8.html" rel="nofollow ugc noopener">g2-standard-8</a><span> (32 GB RAM) machine with a single </span><a href="https://cloud.google.com/blog/products/compute/introducing-g2-vms-with-nvidia-l4-gpus" rel="nofollow ugc noopener">L4 GPU</a><span> attached.</span></p>
<p>Note that there are several factors determining if a model can serve results, key being:</p>
<ol><li><p>the number of model parameters (here 1.5B, 7B, 14B for Qwen2.5-Instruct)</p></li><li><p>the vLLM server argument <samp>--max-model-len</samp> that can optionally set model context length to less than the default value of 16k</p></li><li><p>level of model quantisation, if any</p></li></ol>
<p>Here are some sample results:</p>
<ul><li><p><span>✅ </span><a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" rel="nofollow ugc noopener">Qwen/Qwen2.5-1.5B-Instruct</a><span> is good and easily fits</span></p></li><li><p><span>✅ </span><a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" rel="nofollow ugc noopener">Qwen/Qwen2.5-7B-Instruct</a><span> with <samp>--max-model-len=4156</samp> is good</span></p></li><li><p><span>❌ </span><a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" rel="nofollow ugc noopener">Qwen/Qwen2.5-7B-Instruct</a><span> with default max-model-len of 16k cannot be allocated on endpoint deployment</span></p></li><li><p><span>❌ </span><a href="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct" rel="nofollow ugc noopener">Qwen/Qwen2.5-14B-Instruct</a><span> is too large for the L4 resulting in torch.OutOfMemoryError: CUDA out of memory error</span></p></li><li><p><span>✅ </span><a href="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-AWQ" rel="nofollow ugc noopener">Qwen/Qwen2.5-14B-Instruct-AWQ</a><span> quantised model is good</span></p></li></ul>

<h2 class="header-anchor-post">Selected vLLM tuning options</h2>
<p><span>There are many vLLM engine arguments, which you can see here: </span><a href="https://docs.vllm.ai/en/stable/serving/engine_args.html" rel="nofollow ugc noopener">https://docs.vllm.ai/en/stable/serving/engine_args.html</a></p>
<p>Here are some notable arguments that could have a big impact on serving performance:</p>
<pre><code>--disable-log-stats (Disable logging statistics)

--dtype=float16 OR bfloat16 (For memory savings. Default: “auto”: “auto” will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models. “half” for FP16 | "half" recommended for AWQ quantization.)

--gpu-memory-utilization=0.95 (The fraction of GPU memory to be used for the model executor. Default is 0.9)

--swap-space=XX (CPU swap space size GiB per GPU. Default is 4)

--tensor-parallel-size=N (Number of tensor parallel replicas. Default: 1. Try multi-gpu deployment?)

--artifact-uri="$MODEL_ARTIFACT_URI" (for testing custom GCS models)

--enable-prefix-caching (Improve prefilling phase. Automatic Prefix Caching caches the KV cache of existing queries)

--disable-sliding-window (Disables sliding window, capping to sliding window size.)

--seed (Random seed for operations, useful for reproducibility. Default: 0)</code></pre>

<h2 class="header-anchor-post">Conclusion</h2>
<p>vLLM is a highly efficient and practical framework for deploying LLMs in production, especially when integrated with Google's VertexAI. With features like PagedAttention, continuous batching, speculative decoding, and quantisation support, vLLM can provide exceptional performance, balancing throughput and latency.</p>
<p>Google's custom Docker images for VertexAI simplify model deployment by seamlessly aligning vLLM’s OpenAI-compatible API with VertexAI's requirements, making GCP productionisation straight forward.</p>

<h2 class="header-anchor-post">References</h2>
<ul><li><p><a href="https://github.com/vllm-project/vllm" rel="nofollow ugc noopener">https://github.com/vllm-project/vllm</a></p></li><li><p><a href="https://github.com/vllm-project/production-stack?tab=readme-ov-file#deployment" rel="nofollow ugc noopener">https://github.com/vllm-project/production-stack?tab=readme-ov-file#deployment</a></p></li><li><p><a href="https://docs.vllm.ai/en/stable/" rel="nofollow ugc noopener">https://docs.vllm.ai/en/stable/</a></p></li><li><p><a href="https://sky.cs.berkeley.edu/project/vllm/" rel="nofollow ugc noopener">https://sky.cs.berkeley.edu/project/vllm/</a></p></li><li><p><a href="https://docs.vllm.ai/en/latest/performance/optimization.html" rel="nofollow ugc noopener">https://docs.vllm.ai/en/latest/performance/optimization.html</a></p></li><li><p><a href="https://docs.vllm.ai/en/latest/serving/engine_args.html" rel="nofollow ugc noopener">https://docs.vllm.ai/en/latest/serving/engine_args.html</a></p></li><li><p><a href="https://blog.vllm.ai/2023/06/20/vllm.html" rel="nofollow ugc noopener">https://blog.vllm.ai/2023/06/20/vllm.html</a></p></li><li><p><a href="https://arxiv.org/abs/2309.06180" rel="nofollow ugc noopener">https://arxiv.org/abs/2309.06180</a><span> (PagedAttention paper)</span></p></li><li><p><a href="https://cloud.google.com/vertex-ai" rel="nofollow ugc noopener">https://cloud.google.com/vertex-ai</a></p></li><li><p><a href="https://console.cloud.google.com/artifacts/docker/vertex-ai/us/vertex-vision-model-garden-dockers/pytorch-vllm-serve" rel="nofollow ugc noopener">https://console.cloud.google.com/artifacts/docker/vertex-ai/us/vertex-vision-model-garden-dockers/pytorch-vllm-serve</a></p></li><li><p><a href="https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/" rel="nofollow ugc noopener">https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/</a></p></li><li><p><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference" rel="nofollow ugc noopener">https://www.anyscale.com/blog/continuous-batching-llm-inference</a></p></li><li><p><a href="https://www.hyperstack.cloud/blog/case-study/what-is-vllm-a-guide-to-quick-inference" rel="nofollow ugc noopener">https://www.hyperstack.cloud/blog/case-study/what-is-vllm-a-guide-to-quick-inference</a></p></li><li><p><a href="https://huggingface.co/Qwen" rel="nofollow ugc noopener">https://huggingface.co/Qwen</a></p></li></ul>
