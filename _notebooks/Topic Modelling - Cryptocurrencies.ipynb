{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTopic modelling for currencies.\\n\\nPrereq for data: /Users/dylan/_git/drh-data-scraper/src/0_run_all.sh\\n\\nRun me:\\npython /Users/dylan/_git/drh-cryptopulse/crawlers/coinmarketcap/wf7_temp_currency_topic_modelling.py\\n\\nRef: \\n    https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730\\n    https://de.dariah.eu/tatom/topic_model_python.html\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation #sklearn.decomposition.NMF\n",
    "\n",
    "'''\n",
    "Topic modelling for currencies.\n",
    "\n",
    "Prereq for data: /Users/dylan/_git/drh-data-scraper/src/0_run_all.sh\n",
    "\n",
    "Run me:\n",
    "python /Users/dylan/_git/drh-cryptopulse/crawlers/coinmarketcap/wf7_temp_currency_topic_modelling.py\n",
    "\n",
    "Ref: \n",
    "    https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730\n",
    "    https://de.dariah.eu/tatom/topic_model_python.html\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs():\n",
    "    infolder = '/Users/dylan/_git/drh-cryptopulse/crawlers/coinmarketcap/data_coinmarketcap/'\n",
    "    infile_generic_websites = infolder + '3-generic-website.json'\n",
    "\n",
    "    with open(infile_generic_websites) as f:\n",
    "        json_generic_websites = json.load(f)\n",
    "\n",
    "    websites = list(x for x in json_generic_websites) # if x['_url_desc'] == currency_name)  # Can be mulitple\n",
    "\n",
    "    documents = []\n",
    "    documents_names = []\n",
    "\n",
    "    # Website(s) rows (can be more than 1 for a currency, e.g. bitcoin.com and bitcoin.org)\n",
    "    for website in websites:\n",
    "\n",
    "        exclude_tags = ['a', 'option']\n",
    "        min_content_length = 2\n",
    "        join_string = '. '\n",
    "\n",
    "        # Create joined_content from list of ('tag', 'content text') items\n",
    "        content_lines = website.get('content_lines', None)\n",
    "        if content_lines:\n",
    "            content_lines = filter(lambda x: x[0] not in exclude_tags, content_lines)\n",
    "            content_lines = [x[1] for x in content_lines]\n",
    "            content_lines = filter(lambda x: len(x) > min_content_length, content_lines)\n",
    "            website['content_lines_joined'] = join_string.join(content_lines)\n",
    "\n",
    "            documents.append(website['content_lines_joined'])\n",
    "            documents_names.append('{} - {}'.format(website['_url_desc'], website['_url_website']))\n",
    "\n",
    "    #documents = [x for x in website.get('content_lines_joined', None) if x != None]\n",
    "\n",
    "    return documents_names, documents\n",
    "    \n",
    "\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    result = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_result = \", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(\"Topic %d: \\t%s\" % (topic_idx, topic_result))\n",
    "        result.append(topic_result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_doctopic_relations(method, model, tf, documents_names, feature_names, header=False):\n",
    "    doc_topic_distrib = model.transform(tf)  # https://stackoverflow.com/questions/34429635/topic-modelling-assign-a-document-with-top-2-topics-as-category-label-sklear\n",
    "\n",
    "    components = range(len(model.components_))\n",
    "    components = list(map(lambda x: 'Topic ' + str(x), components))\n",
    "\n",
    "    result = []\n",
    "\n",
    "    if header:\n",
    "        result.append('Method\\tCurrency\\tWebsite\\t' + '\\t'.join(components))\n",
    "\n",
    "    for i in range(len(documents_names)):\n",
    "        document_name = documents_names[i].replace('http:', '').replace('https:', '').replace('/', '')\n",
    "        topic_dist = doc_topic_distrib[i]\n",
    "        topic_dist_pc = list(map(lambda x: str(int(x*100)), topic_dist))\n",
    "        info = '{}\\t{}\\t{}'.format(method, document_name, '\\t'.join(topic_dist_pc))\n",
    "        result.append(info)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "Calculate cosine similarities between different topic distributions.\n",
    "'''\n",
    "def get_doctopic_similarities(model, tf, documents_names):\n",
    "    doc_topic_distrib = model.transform(tf)  # https://stackoverflow.com/questions/34429635/topic-modelling-assign-a-document-with-top-2-topics-as-category-label-sklear\n",
    "\n",
    "    components = range(len(model.components_))\n",
    "    components = list(map(lambda x: 'Topic ' + str(x), components))\n",
    "\n",
    "    matrix = []\n",
    "\n",
    "    for i in range(len(documents_names)):\n",
    "        document_name = documents_names[i].replace('http:', '').replace('https:', '').replace('/', '')\n",
    "        topic_dist = doc_topic_distrib[i]\n",
    "        matrix.append(topic_dist)\n",
    "\n",
    "    cossim = cosine_similarity(np.array(matrix))\n",
    "    return cossim\n",
    "\n",
    "'''\n",
    "Append column names to lefthand column of existing matrix outputted file.\n",
    "'''\n",
    "def append_lefthand_column_to_matrix(filename, column_names):\n",
    "    lines = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    write_column_names = column_names.copy()\n",
    "    write_column_names.insert(0, '') # First row is blank - assuming there is a header with names already\n",
    "\n",
    "    assert len(write_column_names) == len(lines), 'Column names count did not match row/line count'\n",
    "\n",
    "    # TODO: just get column names from 1st row! Don't need to supply them.\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(lines)):\n",
    "            f.write(write_column_names[i] + '\\t' + lines[i])\n",
    "\n",
    "\n",
    "'''\n",
    "Write out d3 graph formatted connections json file.\n",
    "\n",
    "https://bl.ocks.org/mbostock/3750558 (Sticky Force Layout)\n",
    "https://bl.ocks.org/mbostock/950642 (Labeled Force Layout)\n",
    "\n",
    "'''\n",
    "def write_d3_graph_json(filename, cossim_nmf, documents_names, threshold=0.7):\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('{ \"nodes\": [\\n')\n",
    "\n",
    "        first = True\n",
    "        for name in documents_names:\n",
    "            if not first:\n",
    "                f.write(',')\n",
    "            first = False\n",
    "            f.write('{{\"name\": \"{}\", \"group\": 1}}\\n'.format(name))\n",
    "        \n",
    "        f.write('],\\n')\n",
    "        \n",
    "        f.write('\"links\": [\\n')\n",
    "        \n",
    "        first = True\n",
    "        for n in range(len(cossim_nmf)):\n",
    "            for m in range(0, n):\n",
    "                # if n < 6:\n",
    "                #     print(n, m, cossim_nmf[n][m])\n",
    "                if cossim_nmf[n][m] > threshold:\n",
    "                    if not first:\n",
    "                        f.write(',')\n",
    "                    first = False\n",
    "                    f.write('{{\"source\":{},\"target\":{},\"value\":1}}\\n'.format(n, m))\n",
    "\n",
    "        f.write(']}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main\n",
    "'''\n",
    "def main():\n",
    "    \n",
    "    print('Started...')\n",
    "\n",
    "    num_features = 1000\n",
    "    display_num_top_words = 15\n",
    "    n_components = 10 # Topic count\n",
    "\n",
    "    max_docs = 50\n",
    "\n",
    "    outfolder = '/Users/dylan/_git/drh-cryptopulse/crawlers/coinmarketcap/data_coinmarketcap/'\n",
    "\n",
    "    documents_names, documents = get_docs()\n",
    "\n",
    "    documents_names = documents_names[0:max_docs]\n",
    "    documents = documents[0:max_docs]\n",
    "\n",
    "    print('num docs:', len(documents))\n",
    "\n",
    "    # TODO: check out: https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html\n",
    "\n",
    "    # ######################################################################\n",
    "    # TODO: LSA/LSI\n",
    "    # ######################################################################\n",
    "\n",
    "    # See: gensim.models.LsiModel (https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html#topic=2&lambda=0.53&term=)\n",
    "\n",
    "    # ######################################################################\n",
    "    # TODO: PLSA\n",
    "    # ######################################################################\n",
    "\n",
    "\n",
    "    # ######################################################################\n",
    "    # LDA (Latent Dirichlet Allocation) using term counts.\n",
    "    # ######################################################################\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_features, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    print('\\n\\nLDA')\n",
    "    topics_lda = get_topics(lda, tf_feature_names, display_num_top_words)\n",
    "\n",
    "    relations_lda = get_doctopic_relations('lda', lda, tf, documents_names, tf_feature_names)\n",
    "    with codecs.open(outfolder + '_topic_modeling_LDA.tsv', 'w', 'utf-8') as f: \n",
    "        f.write('\\n'.join(relations_lda))\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Doc similarities\n",
    "    cossim_lda = get_doctopic_similarities(lda, tf, documents_names)\n",
    "    np.savetxt(outfolder + '_doc_cossim_LDA.tsv', cossim_lda, delimiter='\\t', fmt='%1.2f', header='\\t'.join(documents_names), comments='')\n",
    "    append_lefthand_column_to_matrix(outfolder + '_doc_cossim_LDA.tsv', documents_names)\n",
    "\n",
    "\n",
    "    # ######################################################################\n",
    "    # NMF (Non-negative Matrix Factorization) using TFIDF values.\n",
    "    # ######################################################################\n",
    "\n",
    "    nmf_min_df = 10 #2\n",
    "\n",
    "    # NMF is able to use tf-idf\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=nmf_min_df, max_features=num_features, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Run NMF\n",
    "    nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "    print('NMF')\n",
    "    topics_nmf = get_topics(nmf, tfidf_feature_names, display_num_top_words)\n",
    "\n",
    "    relations_nmf = get_doctopic_relations('nmf', nmf, tfidf, documents_names, tfidf_feature_names, True)\n",
    "    with codecs.open(outfolder + '_topic_modeling_NMF.tsv', 'w', 'utf-8') as f: \n",
    "        f.write('\\n'.join(relations_nmf))\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Doc similarities\n",
    "    cossim_nmf = get_doctopic_similarities(nmf, tfidf, documents_names)\n",
    "    np.savetxt(outfolder + '_doc_cossim_NMF.tsv', cossim_nmf, delimiter='\\t', fmt='%1.2f', header='\\t'.join(documents_names), comments='')\n",
    "    append_lefthand_column_to_matrix(outfolder + '_doc_cossim_NMF.tsv', documents_names)\n",
    "\n",
    "    write_d3_graph_json(outfolder + '_graph_NMF.json', cossim_nmf, documents_names)\n",
    "\n",
    "\n",
    "    # ######################################################################\n",
    "    # ALL\n",
    "    # ######################################################################\n",
    "\n",
    "    # TEMP: ??\n",
    "\n",
    "    with codecs.open(outfolder + '_topic_modeling_ALL.tsv', 'w', 'utf-8') as f: \n",
    "        f.write('\\n'.join(relations_nmf))\n",
    "        f.write('\\n')\n",
    "\n",
    "    with codecs.open(outfolder + '_topic_modeling_ALL.tsv', 'a', 'utf-8') as f: \n",
    "        f.write('\\n'.join(relations_lda))\n",
    "        f.write('\\n')\n",
    "\n",
    "    print('Finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started...\n",
      "num docs: 50\n",
      "\n",
      "\n",
      "LDA\n",
      "Topic 0: \ttokens, blockchain, platform, buyer, token, waves, ethereum, agreement, parties, assets, company, person, new, applications, use\n",
      "Topic 1: \tbuyer, tokens, agreement, company, software, token, distribution, io, blockchain, contract, block, parties, platform, including, 2017\n",
      "Topic 2: \tethereum, 2016, 2017, platform, blockchain, august, july, new, bitcoin, community, eth, wallet, contract, tokens, user\n",
      "Topic 3: \tblockchain, bitcoin, network, platform, wallet, technology, decentralized, use, new, transactions, digital, development, community, 2017, world\n",
      "Topic 4: \tblockchain, developer, bitcoin, end, buyer, network, tokens, 2017, new, technology, applications, wallet, world, market, decentralized\n",
      "Topic 5: \tlitecoin, days, pending, hours, btc, exchange, nov, site, core, 10, ltc, exchanges, 12, users, using\n",
      "Topic 6: \tassets, digital, ethereum, daa, platform, 2017, 2016, blockchain, manage, various, performance, risk, asset, investors, index\n",
      "Topic 7: \tripple, payments, blockchain, new, transaction, asset, learn, offer, time, ledger, payment, data, banks, use, world\n",
      "Topic 8: \tgolem, neo, monero, network, wallet, decentralized, use, ethereum, 509, 智能经济, neocontract, software, transaction, computing, user\n",
      "Topic 9: \tbitcoin, blockchain, wallet, network, decentralized, litecoin, community, team, cryptocurrency, currency, mining, 2017, block, platform, university\n",
      "NMF\n",
      "Topic 0: \tblockchain, platform, smart, technology, applications, business, development, world, ecosystem, end, decentralized, use, security, contracts, information\n",
      "Topic 1: \twallet, core, download, block, mining, coin, network, miners, currency, users, source, btc, need, use, community\n",
      "Topic 2: \tslack, github, twitter, copyright, trust, web, team, 2017, learn, run, account, address, fast, experience, existing\n",
      "Topic 3: \tbitcoin, cash, peer, com, buy, uses, payment, transactions, money, new, fees, released, low, community, worldwide\n",
      "Topic 4: \tethereum, 2016, 2017, user, network, token, platform, decentralized, risk, based, proof, contract, currencies, including, research\n",
      "Topic 5: \tassets, digital, cash, management, currency, asset, access, risk, sign, blockchains, real, 000, value, security, join\n",
      "Topic 6: \tprivate, transactions, digital, cryptocurrency, network, secure, privacy, community, cash, news, resources, started, currency, users, decentralized\n",
      "Topic 7: \tdata, ledger, power, fees, time, distributed, enables, application, new, peer, companies, market, transfer, business, management\n",
      "Topic 8: \tpayments, payment, network, cost, low, costs, updates, scalable, send, global, money, world, mobile, infrastructure, software\n",
      "Topic 9: \ttokens, token, offer, company, contract, software, new, form, transaction, right, including, time, ethereum, offers, app\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:drh-cryptopulse]",
   "language": "python",
   "name": "conda-env-drh-cryptopulse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
